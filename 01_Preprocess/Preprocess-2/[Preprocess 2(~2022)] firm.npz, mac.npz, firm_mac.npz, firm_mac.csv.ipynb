{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b15f9f7",
   "metadata": {},
   "source": [
    "### 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c574c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연도별로 시총 top 1000에 드는 회사만을 데이터셋에 포함시킨다\n",
    "def AnnualCap(raw_data, start_year, end_year, num_top=1000):\n",
    "    raw_data['date'] = pd.to_datetime(raw_data['date'])\n",
    "    df_top = pd.DataFrame()\n",
    "    \n",
    "    for year in range(start_year, end_year+1): # 매년\n",
    "        # 각 연도에 해당하는 데이터만을 추출\n",
    "        temp = raw_data[raw_data['date'].dt.year == year]\n",
    "        \n",
    "        # 회사(permno)별로 시가총액(me)의 평균을 구한 후, 시총이 큰 순서대로 내림차순 정렬\n",
    "        # 내림차순 정렬된 상태에서 상위 num_top개의 회사를 추출하여 리스트로 만들기\n",
    "        top_list = temp.groupby('permno').mean()[['me']].reset_index().\\\n",
    "                    sort_values(by='me', ascending=False)[:num_top]['permno'].tolist()\n",
    "        # temp에서 상위 num_top개의 회사만을 포함하는 데이터를 추출 후 이를 매년 반복한 데이터와 concat\n",
    "        df_top = pd.concat([df_top, temp[temp['permno'].isin(top_list)]], axis=0)\n",
    "\n",
    "    return df_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9b9b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inf, -Inf 있는 경우 0으로 대체\n",
    "def ind_clean_data(raw_data, scaling_features):\n",
    "    raw_data[scaling_features] = raw_data[scaling_features].astype('float32')\n",
    "    for col in scaling_features:\n",
    "        is_inf = (raw_data[col] == float(\"inf\")) | (raw_data[col] == float(\"-inf\"))\n",
    "        res = sum(is_inf)\n",
    "        if res != 0:\n",
    "            raw_data.loc[is_inf,col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "177f8d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# min max scaling을 하기 위한 함수\n",
    "def my_scaler(scaling_features, train_data, test_data):\n",
    "    '''\n",
    "    scaling_features: min max scaling을 적용할 변수들 리스트\n",
    "    train data로 fit 한 후 \n",
    "    train data와 test data에 scaling을 적용(transform)한다.\n",
    "    '''\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    scaler.fit(train_data[scaling_features])\n",
    "    \n",
    "    train_data[scaling_features] = scaler.transform(train_data[scaling_features])\n",
    "    test_data[scaling_features] = scaler.transform(test_data[scaling_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7a166ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_stationary(macro_data):\n",
    "    d_mac = macro_data.copy()\n",
    "    for col in d_mac.iloc[:,1:].columns:\n",
    "        d_mac[f'd_{col}'] = (d_mac[col] - d_mac[col].shift(1).bfill())/d_mac[col]\n",
    "        d_mac.drop(col, axis=1, inplace=True)\n",
    "    return d_mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f436316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임으로 되어있는 macro 데이터를 넘파이 배열로 변환 후 npz 파일로 저장하는 함수\n",
    "def macro_to_npz(d_mac, filename):\n",
    "    mac_date = d_mac['sasdate'].astype(str).to_numpy()\n",
    "    mac_variables = d_mac.columns[1:].to_numpy()\n",
    "    d_mac_np = d_mac.iloc[:,1:].to_numpy()\n",
    "    np.savez(f'{filename}', date=mac_date, variable=mac_variables, data=d_mac_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16884e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_clean_data(d_mac_data):\n",
    "    d_mac_data.iloc[:,1:] = d_mac_data.iloc[:,1:].astype('float32')\n",
    "    for col in d_mac_data.iloc[:,1:].columns:\n",
    "        is_inf = (d_mac_data[col] == float(\"inf\")) | (d_mac_data[col] == float(\"-inf\"))\n",
    "        res = sum(is_inf)\n",
    "        if res != 0:\n",
    "            d_mac_data.loc[is_inf,col] = 0\n",
    "    d_mac_data.iloc[:,1:] = d_mac_data.iloc[:,1:].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e28d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 refit 하는 것처럼 scaling도 같은 방식으로\n",
    "def re_scaling(df, scaling_features, train_start=1996, train_end=2016, test_size=5):\n",
    "    tmp = df.copy()\n",
    "    tmp['year'] = tmp['sasdate'].dt.year\n",
    "    # scaling test set\n",
    "    new_test = pd.DataFrame()\n",
    "    for i in range(test_size):\n",
    "        start_year = train_start\n",
    "        end_year = train_end + i\n",
    "\n",
    "        train = tmp[(tmp['year']>=start_year) & (tmp['year']<=end_year)]\n",
    "        test = tmp[tmp['year']==end_year+1]\n",
    "\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaler.fit(train[scaling_features])\n",
    "        test[scaling_features] = scaler.transform(test[scaling_features])\n",
    "        new_test = pd.concat([new_test, test], axis=0)\n",
    "    \n",
    "    # scaling train set\n",
    "    full_train = tmp[(tmp['year']>=train_start) & (tmp['year']<=train_end)]\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(full_train[scaling_features])\n",
    "    full_train[scaling_features] = scaler.transform(full_train[scaling_features])\n",
    "\n",
    "    return full_train, new_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed6c25a",
   "metadata": {},
   "source": [
    "# firm.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1db253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "395bca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1990 = pd.read_csv('./data/raw_data/firm-specific/chars60_raw_1990s.csv')\n",
    "raw2000 = pd.read_csv('./data/raw_data/firm-specific/chars60_raw_2000s.csv')\n",
    "raw2010 = pd.read_csv('./data/raw_data/firm-specific/chars60_raw_2010s.csv')\n",
    "raw2020 = pd.read_csv('./data/raw_data/firm-specific/chars60_raw_2020s.csv')\n",
    "\n",
    "raw = pd.concat([raw1990, raw2000, raw2010, raw2020], axis=0)\n",
    "raw['date'] = pd.to_datetime(raw['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cb70dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_top = AnnualCap(raw, 1996, 2021, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "652c50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_features = ['adm',\n",
    "       'bm_ia', 'herf', 'hire', 'me_ia', 'ill', 'maxret', 'mom12m', 'mom1m',\n",
    "       'mom36m', 'mom60m', 'mom6m', 'std_dolvol', 'me', 'dy', 'cinvest',\n",
    "       'nincr', 'pscore', 'acc', 'bm', 'agr', 'alm', 'ato', 'cash', 'cashdebt',\n",
    "       'cfp', 'chcsho', 'chpm', 'chtx', 'depr', 'ep', 'gma', 'grltnoa', 'lev',\n",
    "       'lgr', 'ni', 'noa', 'op', 'pctacc', 'pm', 'rd_sale', 'rdm', 'rna',\n",
    "       'roa', 'roe', 'rsup', 'sgr', 'sp']\n",
    "\n",
    "ind_clean_data(raw_top, scaling_features)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(raw_top[scaling_features])\n",
    "raw_top[scaling_features] = scaler.transform(raw_top[scaling_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e8a0d26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "class Individual_Preprocess:\n",
    "    def __init__(self, raw_data, filename):\n",
    "        self.filename = filename\n",
    "        self.raw_data = raw_data\n",
    "        self.start = \"1996-01-01\"\n",
    "        self.end = \"2021-12-31\"\n",
    "\n",
    "        self.make_same_length()\n",
    "        self.ind_to_npz()\n",
    "        \n",
    "    def make_same_length(self):\n",
    "        lis = pd.date_range(start=self.start, end=self.end, freq='M') \n",
    "        lis = [date_obj.strftime('%Y-%m-%d') for date_obj in lis]\n",
    "        lis = pd.DataFrame({'date':lis})\n",
    "        lis['date'] = pd.to_datetime(lis['date'])\n",
    "        self._lis = lis\n",
    "\n",
    "        unq_date = lis['date'].apply(lambda x: int(x.strftime('%Y%m%d')))\n",
    "        self._unq_date = unq_date.to_numpy()\n",
    "\n",
    "        raw_merged = pd.DataFrame()\n",
    "        for perm in self.raw_data['permno'].unique():\n",
    "            tmp = self.raw_data[self.raw_data['permno']==perm]\n",
    "            tmp = tmp.sort_values(by='date')\n",
    "            tmp_merged = pd.merge(tmp, lis, on=['date'], how='right')\n",
    "            tmp_merged['permno'] = perm\n",
    "            raw_merged = pd.concat([raw_merged, tmp_merged], axis=0)\n",
    "        \n",
    "        raw_merged = raw_merged.sort_values(by=['date','permno'])\n",
    "        self._raw_merged = raw_merged\n",
    "    \n",
    "    def ind_to_npz(self):\n",
    "        \n",
    "        self._raw_merged = self._raw_merged.drop(['permno','ticker','gvkey','sic','exchcd',\n",
    "                                                  'shrcd','ffi49','year','date'], axis=1)\n",
    "        self._raw_merged = self._raw_merged.fillna(-99.99)\n",
    "        \n",
    "        num_dates = self._lis.shape[0]\n",
    "        num_permno = self.raw_data['permno'].nunique()\n",
    "        num_ind_features = self._raw_merged.shape[1] \n",
    "        print('num_ind_features',num_ind_features)\n",
    "        \n",
    "        raw_merged_np = self._raw_merged.to_numpy()\n",
    "        raw_merged_np = raw_merged_np.reshape(num_dates, num_permno, num_ind_features)\n",
    "        \n",
    "        ind_variables = self._raw_merged.columns.to_numpy()\n",
    "        \n",
    "        np.savez(f'{self.filename}', date=self._unq_date, variable=ind_variables, data=raw_merged_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "75b29058",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_ind_features 49\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Individual_Preprocess at 0x7fbd682125b0>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Individual_Preprocess(raw_top, 'firm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "69fbd429",
   "metadata": {},
   "outputs": [],
   "source": [
    "firm = np.load('firm.npz', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1d6332",
   "metadata": {},
   "source": [
    "# mac.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f56939",
   "metadata": {},
   "source": [
    "- macro 데이터도 firm-specific 데이터와 마찬가지로 변수마다 업데이트되는 주기가 다름\n",
    "- 어떤 변수는 3달 후에 알 수 있고(lag3_vars), 어떤 변수는 2달 후에 알 수 있고(lag2_vars) 등등..\n",
    "- 이런 변수별 업데이트 주기를 반영해야 함 \n",
    "- 이때, 현재 시점에서 알 수 있는 가장 최근 값을 사용하도록 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3d57e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mac = pd.read_csv('./raw_data/macroeconomic/fred_macro_2022-05.csv')\n",
    "mac['sasdate'] = pd.to_datetime(mac['sasdate'])\n",
    "\n",
    "lag3_vars = ['CMRMTSPLx', 'HWI', 'HWIURATIO', 'ACOGNO', 'BUSINVx', 'ISRATIOx', 'NONREVSL', 'CONSPI', \n",
    "             'S&P div yield', 'DTCOLNVHFNM', 'DTCTHFNM']\n",
    "lag5_vars = ['S&P PE ratio']\n",
    "lag2_vars = mac.columns.drop(['sasdate']+lag3_vars+lag5_vars).tolist()\n",
    "\n",
    "# 변수별로 업데이트 주기를 반영해서 lag 시키는 함수\n",
    "def lag_vars(macro_data):\n",
    "    macro_data[lag2_vars] = macro_data[lag2_vars].shift(2).bfill()\n",
    "    macro_data[lag3_vars] = macro_data[lag3_vars].shift(3).bfill()\n",
    "    macro_data[lag5_vars] = macro_data[lag5_vars].shift(5).bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95cba73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_vars(mac)\n",
    "macro_clean_data(mac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56f50ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mac_all = mac[(mac['sasdate'] >= '1996-01-01') & (mac['sasdate'] <= '2022-12-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8461feb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 비정상계열인 데이터를 정상계열로 만들어서 쓰기 위해 차분 후 나눔 (return 계산하듯이)\n",
    "d_mac = make_stationary(mac)\n",
    "macro_clean_data(d_mac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "786a27ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "d_mac_train = d_mac[(d_mac['sasdate'] >= '1996-01-01') & (d_mac['sasdate'] <= '2016-12-31')]\n",
    "d_mac_test = d_mac[(d_mac['sasdate'] >= '2017-01-01') & (d_mac['sasdate'] <= '2022-12-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ff77576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_scaler(scaling_features, train_data, test_data):\n",
    "    '''\n",
    "    scaling_features: min max scaling을 적용할 변수들 리스트\n",
    "    train data로 fit 한 후 \n",
    "    train data와 test data에 scaling을 적용(transform)한다.\n",
    "    '''\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    scaler.fit(train_data[scaling_features])\n",
    "    \n",
    "    train_data[scaling_features] = scaler.transform(train_data[scaling_features])\n",
    "    test_data[scaling_features] = scaler.transform(test_data[scaling_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fc70fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe30ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_mac_all = pd.concat([d_mac_train, d_mac_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7755d4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 반드시 na가 없어야 함\n",
    "d_mac_all.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b80e724d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(316, 128)\n"
     ]
    }
   ],
   "source": [
    "print(d_mac_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b6bc615",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# npz 파일로 저장\n",
    "macro_to_npz(d_mac_all, 'mac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0be174fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# npz 파일 불러오기\n",
    "mac = np.load('mac.npz', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4cda7c",
   "metadata": {},
   "source": [
    "# firm_mac.npz\n",
    "---\n",
    "- firm-specific 변수와 차분한 macro 변수를 합침"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e0f456ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_top['month'] = raw_top['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5f187fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_mac_all['year'] = d_mac_all['sasdate'].dt.year\n",
    "d_mac_all['month'] = d_mac_all['sasdate'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e17d88af",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_d_mac_all = pd.merge(raw_top, d_mac_all, on=['year','month'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f87e5f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_d_mac_all.drop(['month','sasdate'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "41fa651f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_ind_features 176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Individual_Preprocess at 0x7f8768cae1f0>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Individual_Preprocess(raw_d_mac_all, 'firm_mac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3c9b311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "firm_mac = np.load('firm_mac.npz', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3710841",
   "metadata": {},
   "source": [
    "# firm_mac.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a3799789",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_d_mac_all.to_csv('firm_mac.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeac8895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
